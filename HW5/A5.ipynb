{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS 583 Assignment 5: YOLO Object Detection on PASCAL VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷 M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm XXX, XXXXXXXXXX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Get Data\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `VOCdevkit_2007.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/1jWpMbjxFl1JALOcwRMg3dl44UAURQ2XB/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `VOCdevkit_2007.zip` \n",
    "\n",
    "+ `VOC2007` : 包含了train/val的所有圖片\n",
    "+ `VOC2007test` : 包含了test的所有圖片\n",
    "\n",
    "其中`train`的圖片 2501 張，`val`的圖片 2510 張，`test` 的圖片 4950 張。\n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice: Please put \"VOCdevkit_2007\" folder under data folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq ./drive/MyDrive/VOCdevkit_2007.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from src.resnet_yolo import resnet50\n",
    "from yolo_loss import YoloLoss\n",
    "from src.dataset import VocDetectorDataset\n",
    "from src.eval_voc import evaluate, test_evaluate\n",
    "from src.predict import predict_image\n",
    "from src.config import VOC_CLASSES, COLORS\n",
    "from kaggle_submission import write_csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO network hyperparameters\n",
    "B = 2  # number of bounding box predictions per cell\n",
    "S = 14  # width/height of network output grid (larger than 7x7 from paper since we use a different network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Yolo we will rely on a pretrained classifier as the backbone for our detection network. PyTorch offers a variety of models which are pretrained on ImageNet in the [`torchvision.models`](https://pytorch.org/docs/stable/torchvision/models.html) package. In particular, we will use the ResNet50 architecture as a base for our detector. This is different from the base architecture in the Yolo paper and also results in a different output grid size (14x14 instead of 7x7).\n",
    "\n",
    "Models are typically pretrained on ImageNet since the dataset is very large (> 1 million images) and widely used. The pretrained model provides a very useful weight initialization for our detector, so that the network is able to learn quickly and effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_network_path = None #'checkpoints/best_detector.pth' \n",
    "pretrained = True\n",
    "\n",
    "# use to load a previously trained network\n",
    "if load_network_path is not None:\n",
    "    print('Loading saved network from {}'.format(load_network_path))\n",
    "    net = resnet50().to(device)\n",
    "    net.load_state_dict(torch.load(load_network_path))\n",
    "else:\n",
    "    print('Load pre-trained model')\n",
    "    net = resnet50(pretrained=pretrained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 24\n",
    "\n",
    "# Yolo loss component coefficients (as given in Yolo v1 paper)\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Pascal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset loader also using a variety of data augmentation techniques including random shift, scaling, crop, and flips. Data augmentation is slightly more complicated for detection datasets since the bounding box annotations must be kept consistent throughout the transformations.\n",
    "\n",
    "Since the output of the detector network we train is an SxSx(B*5+C), we use an encoder to convert the original bounding box coordinates into relative grid bounding box coordinates corresponding to the expected output. We also use a decoder which allows us to convert the opposite direction into image coordinate bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice: Please put \"VOCdevkit_2007\" folder under data folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_root_train = 'data/VOCdevkit_2007/VOC2007/JPEGImages/'\n",
    "annotation_file_train = 'data/voc2007train.txt'\n",
    "\n",
    "train_dataset = VocDetectorDataset(root_img_dir=file_root_train,dataset_file=annotation_file_train,train=True, S=S)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "print('Loaded %d train images' % len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_root_val = 'data/VOCdevkit_2007/VOC2007/JPEGImages/'\n",
    "annotation_file_val = 'data/voc2007val.txt'\n",
    "\n",
    "val_dataset = VocDetectorDataset(root_img_dir=file_root_val,dataset_file=annotation_file_val,train=False, S=S)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False,num_workers=2)\n",
    "print('Loaded %d val images' % len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = YoloLoss(S, B, lambda_coord, lambda_noobj)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = np.inf\n",
    "learning_rate = 1e-3\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    \n",
    "    # Update learning rate late in training\n",
    "    if epoch == 30 or epoch == 40:\n",
    "        learning_rate /= 10.0\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    \n",
    "    print('\\n\\nStarting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "    print('Learning Rate for this epoch: {}'.format(learning_rate))\n",
    "    \n",
    "    total_loss = collections.defaultdict(int)\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = (item.to(device) for item in data)\n",
    "        images, target_boxes, target_cls, has_object_map = data\n",
    "        pred = net(images)\n",
    "        loss_dict = criterion(pred, target_boxes, target_cls, has_object_map)\n",
    "        for key in loss_dict:\n",
    "            total_loss[key] += loss_dict[key].item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict['total_loss'].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            outstring = 'Epoch [%d/%d], Iter [%d/%d], Loss: ' % ((epoch+1, num_epochs, i+1, len(train_loader)))\n",
    "            outstring += ', '.join( \"%s=%.3f\" % (key[:-5], val / (i+1)) for key, val in total_loss.items() )\n",
    "            print(outstring)\n",
    "    \n",
    "    # evaluate the network on the val data\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        val_aps = evaluate(net, val_dataset_file=annotation_file_val, img_root=file_root_val)\n",
    "        print(epoch, val_aps)\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        net.eval()\n",
    "        for i, data in enumerate(val_loader):\n",
    "            data = (item.to(device) for item in data)\n",
    "            images, target_boxes, target_cls, has_object_map = data\n",
    "            \n",
    "            pred = net(images)\n",
    "            loss_dict = criterion(pred, target_boxes, target_cls, has_object_map)\n",
    "            val_loss += loss_dict['total_loss'].item()\n",
    "        val_loss /= len(val_loader)\n",
    "    \n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print('Updating best val loss: %.5f' % best_val_loss)\n",
    "        torch.save(net.state_dict(),'checkpoints/best_detector.pth')\n",
    "    \n",
    "    if (epoch+1) in [5, 10, 20, 30, 40]:\n",
    "        torch.save(net.state_dict(),'checkpoints/detector_epoch_%d.pth' % (epoch+1))\n",
    "\n",
    "    torch.save(net.state_dict(),'checkpoints/detector.pth')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "# select random image from val set\n",
    "image_name = random.choice(val_dataset.fnames)\n",
    "image = cv2.imread(os.path.join(file_root_val, image_name))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print('predicting...')\n",
    "result = predict_image(net, image_name, root_img_directory=file_root_val)\n",
    "for left_up, right_bottom, class_name, _, prob in result:\n",
    "    color = COLORS[VOC_CLASSES.index(class_name)]\n",
    "    cv2.rectangle(image, left_up, right_bottom, color, 2)\n",
    "    label = class_name + str(round(prob, 2))\n",
    "    text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
    "    p1 = (left_up[0], left_up[1] - text_size[1])\n",
    "    cv2.rectangle(image, (p1[0] - 2 // 2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]),\n",
    "                  color, -1)\n",
    "    cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate on Validation\n",
    "\n",
    "To evaluate detection results we use mAP (mean of average precision over each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aps = evaluate(net, val_dataset_file=annotation_file_val, img_root=file_root_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell added to get intermediate mAP values for students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_paths = ['checkpoints/detector_epoch_%d.pth' % epoch for epoch in [5, 10, 20, 30, 40]]+['checkpoints/detector.pth']\n",
    "for load_network_path in network_paths:\n",
    "    print('Loading saved network from {}'.format(load_network_path))\n",
    "    net_loaded =  resnet50().to(device)\n",
    "    net_loaded.load_state_dict(torch.load(load_network_path))\n",
    "    evaluate(net_loaded, val_dataset_file=annotation_file_val,img_root=file_root_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle submission (85%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Result\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/a508522480af44b0bc9e1299b6f28768).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. Click the folder icon in the left hand side of Colab.\n",
    "2. Right click \"result.csv\". Select \"Download\"\n",
    "3. To kaggle. Click \"Submit Predictions\"\n",
    "4. Upload the result.csv\n",
    "5. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard.\n",
    "\n",
    "---\n",
    "\n",
    "預測`test`並將結果上傳至Kaggle。[**連結**](https://www.kaggle.com/t/a508522480af44b0bc9e1299b6f28768)\n",
    "\n",
    "執行完畢此區的程式碼後，會將`test`預測完的結果存下來。\n",
    "\n",
    "上傳流程\n",
    "1. 點選左側選單最下方的資料夾圖示\n",
    "2. 右鍵「result.csv」\n",
    "3. 點選「Download」\n",
    "4. 至連結網頁點選「Submit Predictions」\n",
    "5. 將剛剛下載的檔案上傳\n",
    "6. 系統會計算並公布其中50%資料的正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_test = 'data/VOCdevkit_2007/VOC2007test/JPEGImages/'\n",
    "file_test = 'data/voc2007test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the test_evaluate function, you will obtain predictions for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_submission = test_evaluate(net, test_dataset_file=file_test, img_root=root_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The write_csv function will use preds_submission to write into a CSV file called 'result.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(preds_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15%)\n",
    "In your report, please include:\n",
    "\n",
    "a. A brief discussion on your implementation.\n",
    "\n",
    "b. Report the best train and validation accuracy in all of your experiments and discuss\n",
    "any strategies or tricks you’ve employed.\n",
    "\n",
    "c. Report the results for extra credits and also provide a discussion, if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (15%)\n",
    "\n",
    "• Pick a fun video like [**this one**](https://www.youtube.com/watch?v=xZGahvrep3o), run your detector on it (a subset of frames would be\n",
    "OK), and produce a video showing your results.\n",
    "\n",
    "• Try to replace the provided pre-trained network with a different one and train with\n",
    "the YOLO loss on top to attempt to get better accuracy.\n",
    "\n",
    "• Or any other methods that you try to improve the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aaa478f9632825e83f6a2247407c7a2930de96a6810af7910643e423346524f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
